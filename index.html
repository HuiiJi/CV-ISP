<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<html>
<head>
	<title>CV-ISP</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">CV-ISP: NN based ISP on extreme condition for CV-tasks</span>
		<table align=center width=1000px>
			<table align=center width=1000px cellspacing="5px">
                <br>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:20px"><a href="https://huiiji.github.io/" target=”_blank”>Ji Hui<sup>1</sup>*</a></span>
						</center>
					</td>
				</tr>
			</table>
            <table align=center width="100%">
                <br>
                <tr>
                    <td align=center width="100%">
						<center>
							<h5>
								<i>Recorded in 2023.05</i>
							</h5>
						</center>
                        <center>
                            <span style="font-size:12px">
                                <b>*</b> Document some concepts about CV-ISP <br>
                            </span>
                        </center>
                    </td>
                </tr>
            </table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href='https://github.com/Gatedip/GDIP-Yolo' target="_blank">[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
			<br>
            <table align=center width=800px>
                <tr>
                    <td width=260px>
                        <center>
                            <img class="round" style="width:1200px" src="assets/cvisp-background.png"/>
                        </center>
                    </td>
                </tr>
            </table>
		</table>
	</center>


	<hr>

	<table align=center width=850px>
		<center><h1>Motivation</h1></center>
		<tr>
			<td>
				<ul>
					The proposed scenario of CV-ISP is in the pan-robot field, and the vision sensor and data flow need to consider the scenario of ISP For CV. The ultimate goal of HV-ISP is to improve subjective visual evaluation to conform to the human eye; For machine vision, the image quality of HV-ISP is not necessarily suitable for downstream CV tasks such as detection, segmentation, etc.
					<br><br>
					The ultimate goal of CV-ISP is to effectively improve the indicators under normal/extreme visual perception conditions for common downstream tasks, so CV-ISP's technical route exploration and expected technical goals are particularly important.
					<br><br>
				
				</center>
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<table align=center width=850px>
		<center><h1>Overview</h1></center>
		<!-- <p align=center> To be updated</p> -->
		<tr>
			<td>
				<ul>
					<li>Share some <b>paper</b> research and <b>understanding</b> about CV-ISP.</li>
					<figure align=center>
							<img style="width:1000px" src="assets/cvisp-papersum.png"/>
							<figcaption align="center">Paper and idea
							</figcaption>
					</figure>
					
					<li>According to the experimental direction in the paper, I summarized the possible specific <b>technical routes</b> for CV-ISP.</li>
					<figure align=center>
							<img style="width:600px" src="assets/cvisp-route.png"/>
							<figcaption align="center">technical direction
							</figcaption>
					</figure>
					
					<li><b>The basic implementation of CV-ISP needs to be NN as the backbone and RGB as the output to adapt to the CV task of the general versionr, so it's efficient to achieve Programme c.</li>
					<figure align=center>
							<img style="width:400px" src="assets/cvisp-NN.jpg"/>
							<figcaption align="center">efficient technical programme
							</figcaption>
					</figure>
				
			</td>
		</tr>
		
	</table>
	<br>

	<hr>

	<table align=center width=850px>
		<center><h1>NN Modules</h1></center>
		<tr>
			<td> <p>Using below CNN-Network, it's available to build the backbone for CV-ISP:</p>
				<ul>
					<li> <b>SOTA Networks on low-level vision. </b>  
						<figure align=center>
							<img style="width:600px" src="assets/AI-enhence.png"/>
							<figcaption align="center">SOTA Networks sum
							</figcaption>
						</figure>
					<br>
					<li> <b>CV-ISP/ISP pipeline project for reference.</b> 
						<figure align=center>
							<img style="width:600px" src="assets/cvisp-reference.png"/>
							<figcaption align="center">CV-ISP pipeline project
							</figcaption>
						</figure>
					<br>

					<li> <b>default ISP project:</b> 
						<tr>
							<td align=center width=150px>
								<center>
									<span style="font-size:20px"><a href="https://github.com/QiuJueqin/fast-openISP" target=”_blank”>Fast-openisp</a></span>
								</center>
							</td>
						</tr>
						
						<tr>
							<td align=center width=150px>
								<center>
									<span style="font-size:20px"><a href="https://github.com/cruxopen/openISP" target=”_blank”>openisp</a></span>
								</center>
							</td>
						</tr>

				</ul>
			
			</td>
		</tr>
	</table>

	<br>

	<hr>
	<table align=center width=850px>
		<center><h1>Quantitative Analysis</h1></center>
		<tr>
			<td>
				<ul>
					<p align=center style="font-size: 20px">Extreme Low Light</p>
					<figure align =center>
						<img style="width:600px" src="assets/eos.png"/>
						<figcaption align="center" style="font-size: 13px">Algolux EOS software, an end2end dectector for extreme condition,the ceiling that represents the effect.
						</figcaption>
					</figure>

					<p align=center style="font-size: 20px">Low-lighting setting</p>
					<figure align =center>
						<img style="width:600px" src="assets/hvisp-cvisp.png"/>
						<figcaption align="center" style="font-size: 13px">  Our team's CV-ISP-output for yolov5 in extreme low light testset captured by dark lab.
							<br><br>
							<b>NOTE:</b> The CV-ISP pipeline total cover <20MBytes and nearly 150G MACs@1080P-fp32. 
							(show that could be ported to AI computing equipment with low computing power) 
						</figcaption>
					</figure>
				</ul>
			
			</td>
		</tr>
	</table>

	<hr>
	<table align=center width=850px>
		<center><h1>Training</h1></center>
		<tr>
			<td>
				<ul>
					<p>Record the intermediate process of training a CV-ISP.
					</p>
					<br>
					<figure>
						<img style="width:800px; margin-right: 100px" src="assets/cvisp-training.gif"/>
						<figcaption align="center"><b>Note:</b> Training for improving mAP for detection
						</figcaption>
					</figure>
				</ul>
			
			</td>
		</tr>
	</table>

	<br>


	<!-- <center> <h3> Distance between Adjacent Buildings </h3>
	<img class="round" style="width:1000px" src="assets/images/DistanceModule.png"/>
	<p>This module provide us the distance between two adjacent buildings. We sampled the images from the videos captured by UAV and perform panoptic segmentation using state-of-art deep learning model, eliminating vegetation (like trees) from the images. The masked images are then fed to a state-of-the art image-based 3D reconstruction library which outputs a dense 3D point cloud. We then apply RANSAC for fitting planes between the segmented structural point cloud. Further, the points are sampled on these planes to calculate the distance between the adjacent buildings at different locations.</p>
	</center>
	<br> -->


	<!-- <center> <h3> Results: Distance between Adjacent Buildings </h3>
	<img class="round" style="width:800px" src="assets/images/Results-DistanceModule-1.png"/>
	<p> Sub-figures (a), (b) and (c) and (d), (e) and (f) represent the implementation of plane fitting using piecewise-RANSAC in different views for two subject buildings.</p>
	
	</center>
	<br> -->




	<!-- <center> <h3> Plan Shape and Roof Area Estimation </h3>
	<img class="round" style="width:1000px" src="assets/images/PlanShape&RoofareaEstimation.png"/>
	<p>This module provides information regarding the shape and roof area of the building. We segment the roof using a state-of-the-art semantic segmentation deep learning model. We also subjected the input images to a pre-processing module that removes distortions from the wide-angle images. Data augmentation was used to increase the robustness and performance. Roof Area was calculated using the focal length of the camera, the height of the drone from the roof and the segmented mask area in pixels.
</p>
	</center>
	<br>



	<center> <h3> Results: Plan Shape and Roof Area Estimation </h3>
	<img class="round" style="width:800px" src="assets/images/Results-PlanShape&RoofareaEstimation-1.png"/>
	<p>This figure represents the roof segmentation results for 4 subject buildings.</p>
	
	</center>
	<br>


	<center> <h3> Roof Layout Estimation </h3>
	<img class="round" style="width:1000px" src="assets/images/RoofLaoutEstimation.png"/>
	<p>This module provides information about the roof layout. Since it is not possible to capture the whole roof in a single frame specially in the case of large sized buildings, we perform large scale image stitching of partially visible roofs followed by NSE detection and roof segmentation.

</p>
	</center>
	<br> -->



	<!-- <center> <h3> Results: Roof Layout Estimation </h3>
	<img class="round" style="width:800px" src="assets/images/imagestitchingoutput.jpeg"/>
	<p>Stitched Image</p>
	<img class="round" style="width:800px" src="assets/images/roofmask.png"/>
	<p>Roof Mask</p>
	<img class="round" style="width:800px" src="assets/images/objectmask.png"/>
	<p>Object Mask</p>
	
	</center>
	<br> -->




<!--

	<center> <h1> Resources </h1>
	<table width="100%" style="margin: 20pt auto; text-align: center;">
	      <tr>
				<td width="33%" valign="middle">
				    <center>
					<a href="https://github.com/atmacvit/meronymnet" target="_blank"
					   class="imageLink"><img src="./resources/Octocat.png" ,=, width="75%" /></a><br /><br />
					<a href="https://github.com/atmacvit/meronymnet" target="_blank">Code</a>
				    </center>
				</td>
				<td width="33%" valign="middle">
				    <center>
					<a href="https://drive.google.com/file/d/1NnY4tcV1wnlSWMzT_Ae6hH6v5l8GCIrX/view?usp=sharing" target="_blank"
					   class="imageLink"><img src="./resources/Paper_crop.png" ,=, width="75%" /></a><br /><br />
					<a href="https://drive.google.com/file/d/1NnY4tcV1wnlSWMzT_Ae6hH6v5l8GCIrX/view?usp=sharing" target="_blank">Paper</a>
				    </center>

				</td>


			       <td width="33%" valign="middle">
				    <center>
					<a href="" target="_blank"
					   class="imageLink"><img src="./resources/Supp_crop.png" ,=, width="75%" /></a><br /><br />
					<a href="" target="_blank">Supplementary</a>
				    </center>
				</td>
	      </tr>
    	</table>
	</center>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br> -->
<hr>
<center> <h1> Contact </h1>
	<p>If you have any questions, please reach out to me.o(*￣▽￣*)ブ.</p>
	</center>
</body>
</html>
